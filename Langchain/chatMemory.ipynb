{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7516d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini_api =\"AIzaSyDaM0twsGt6Rv5M2pY4ze4lZlKc7IQWuiQ\"\n",
    "\n",
    "# Directly pass API key to avoid ADC errors\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key= gemini_api,\n",
    "    temperature = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6797ff4",
   "metadata": {},
   "source": [
    "ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681cc4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c107d280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages= True)\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my name is James\"},  # user message\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
    "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
    "    {\"output\": \"Very cool!\"}  # AI response\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9eccb",
   "metadata": {},
   "source": [
    "## OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d385d3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7454832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_15472\\3079832730.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm =llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55468916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is my name', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Your name is James! You told me that right at the beginning of our chat. It's nice to meet you, James!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='what is my name', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Your name is James! You told me that right at the beginning of our chat. It's nice to meet you, James!\", additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name',\n",
       " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is my name', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Your name is James! You told me that right at the beginning of our chat. It's nice to meet you, James!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is my name', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Your name is James! You told me that right at the beginning of our chat. It's nice to meet you, James!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is my name', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Your name is James! I remember you told me that right when we started talking. It's still nice to chat with you, James!\", additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"Your name is James! I remember you told me that right when we started talking. It's still nice to chat with you, James!\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\":\"what is my name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb008b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, ChatPromptTemplate\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "     MessagesPlaceholder(variable_name='history'),\n",
    "     HumanMessagePromptTemplate.from_template(\"{query}\")]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "799c216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c2f9c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e71915fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it's good to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--56f11ea3-468e-4175-9da7-62816f519243-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 50, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 17}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bdf372d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I apologize, but as an AI, I don't have a memory of past conversations or access to your personal information. I don't know your name.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--a866f0a2-9f79-4042-80a9-bafece80116f-0', usage_metadata={'input_tokens': 15, 'output_tokens': 33, 'total_tokens': 104, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 56}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_1234\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "458e3422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--7f566688-71a3-430f-909b-fc623d23d89b-0', usage_metadata={'input_tokens': 41, 'output_tokens': 5, 'total_tokens': 74, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 28}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3f8528b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Okay, thanks for letting me know, James! So, your age is 21. Got it.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--f4d3e7e0-fe52-483a-8a50-76d33a40e35e-0', usage_metadata={'input_tokens': 110, 'output_tokens': 22, 'total_tokens': 153, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 21}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"my age is 21?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c9a888c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your age is 21, James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--461ac989-4f89-4aa4-885a-57904881c68a-0', usage_metadata={'input_tokens': 140, 'output_tokens': 9, 'total_tokens': 176, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 27}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my age again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf2ca90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b218f62",
   "metadata": {},
   "source": [
    "ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41abcb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_15472\\3216785012.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9dd9230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc237931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"I'm sorry, I don't know your name. We haven't discussed it in our conversation so far! My memory of our chat only includes the messages we've exchanged about conversational memory types.\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\": \"what is my name again?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ba5c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "335ea6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdd7cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ca6103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n",
      "Initializing BufferWindowMessageHistory with k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it's good to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--05ab5a60-63ac-459d-8b2e-56e6d6535940-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 50, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 17}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e5c6628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history\n",
    "\n",
    "# manually insert history\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k4\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "092a1f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know your name. As an AI, I don't store personal information about you or remember details from past conversations.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--ec0179c1-f9d5-412e-b205-75c0c713679e-0', usage_metadata={'input_tokens': 56, 'output_tokens': 28, 'total_tokens': 144, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 60}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e521a78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n",
      "Initializing BufferWindowMessageHistory with k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it's good to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--f11d4a6f-221a-4b8b-af9f-c953c0578934-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 50, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 17}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99009c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96b3eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--f93de47f-4174-45d6-8556-5a73cebd5746-0', usage_metadata={'input_tokens': 133, 'output_tokens': 5, 'total_tokens': 175, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 37}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e5775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83ccfed7",
   "metadata": {},
   "source": [
    "ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62ecdc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_15472\\988334424.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c3a5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bafde98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello there my name is James\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "New summary:\n",
      "James introduces himself to the AI. The AI introduces itself as a large language model trained by Google, designed for helpful conversations, answering questions, and generating creative content, and then asks James how he is and what he'd like to discuss.\n",
      "Human: I am researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces himself to the AI. The AI introduces itself as a large language model trained by Google, designed for helpful conversations, answering questions, and generating creative content, and then asks James how he is and what he'd like to discuss. James states he is researching different types of conversational memory. The AI acknowledges this as a fascinating and important topic, defining conversational memory as the ability to recall and utilize previously discussed information for natural dialogue. It then details three types: short-term (or working memory), which for AI is handled by a \"context window\" for immediate recall of recent turns; long-term (or episodic memory for AI), which involves recalling information from earlier in a single conversation or from past, separate conversations, often requiring external mechanisms like databases, summarization, or user profiles; and semantic memory (general knowledge), which is the AI's pre-trained understanding of facts and concepts. The AI concludes that improving conversational memory is crucial for making AI assistants more natural and helpful, and asks James if there's a particular aspect he's most interested in.\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces himself to the AI. The AI introduces itself as a large language model trained by Google, designed for helpful conversations, answering questions, and generating creative content, and then asks James how he is and what he'd like to discuss. James states he is researching different types of conversational memory. The AI acknowledges this as a fascinating and important topic, defining conversational memory as the ability to recall and utilize previously discussed information for natural dialogue. It then details three types: short-term (or working memory), which for AI is handled by a \"context window\" for immediate recall of recent turns; long-term (or episodic memory for AI), which involves recalling information from earlier in a single conversation or from past, separate conversations, often requiring external mechanisms like databases, summarization, or user profiles; and semantic memory (general knowledge), which is the AI's pre-trained understanding of facts and concepts. The AI concludes that improving conversational memory is crucial for making AI assistants more natural and helpful, and asks James if there's a particular aspect he's most interested in. James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI confirms these are common implementations of short-term memory, explaining that `ConversationBufferMemory` stores the **entire conversation transcript**, providing full context but risking context window limits, increased cost, and latency in longer dialogues. In contrast, `ConversationBufferWindowMemory` stores only the **last `N` turns** (a \"sliding window\"), which efficiently manages the context window and focuses on recent information, but at the cost of losing older context. The AI summarizes that the former is like a full, growing transcript (good for short chats, prone to limits), while the latter is a sliding window of recent turns (essential for long chats, but loses older info), and asks James if he's interested in a specific framework or the conceptual trade-offs.\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces himself to the AI. The AI introduces itself as a large language model trained by Google, designed for helpful conversations, answering questions, and generating creative content, and then asks James how he is and what he'd like to discuss. James states he is researching different types of conversational memory. The AI acknowledges this as a fascinating and important topic, defining conversational memory as the ability to recall and utilize previously discussed information for natural dialogue. It then details three types: short-term (or working memory), which for AI is handled by a \"context window\" for immediate recall of recent turns; long-term (or episodic memory for AI), which involves recalling information from earlier in a single conversation or from past, separate conversations, often requiring external mechanisms like databases, summarization, or user profiles; and semantic memory (general knowledge), which is the AI's pre-trained understanding of facts and concepts. The AI concludes that improving conversational memory is crucial for making AI assistants more natural and helpful, and asks James if there's a particular aspect he's most interested in. James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI confirms these are common implementations of short-term memory. Regarding `ConversationBufferMemory`, the AI confirms it stores the **entire, unedited transcript** of the conversation from the very beginning, like a complete log, to ensure the AI has access to every piece of information exchanged. However, it reiterates and elaborates on the significant considerations: the finite \"context window\" of LLMs means older parts are truncated if the transcript exceeds it (likened to fitting a long book into a small reading frame); increased cost due to more tokens processed; higher latency from processing larger amounts of text; and the potential for irrelevance as older, unneeded information is still sent, making it harder for the model to focus. Therefore, it's best suited for shorter, focused conversations where full context is genuinely needed and risks are minimal. In contrast, `ConversationBufferWindowMemory` stores only the **last `N` turns** (a \"sliding window\"), which efficiently manages the context window and focuses on recent information, but at the cost of losing older context. The AI summarizes that the former is like a full, growing transcript (good for short chats, prone to limits), while the latter is a sliding window of recent turns (essential for long chats, but loses older info), and asks James if he's interested in a specific framework or the conceptual trade-offs.\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
       " 'history': 'James introduces himself to the AI. The AI introduces itself as a large language model trained by Google, designed for helpful conversations, answering questions, and generating creative content, and then asks James how he is and what he\\'d like to discuss. James states he is researching different types of conversational memory. The AI acknowledges this as a fascinating and important topic, defining conversational memory as the ability to recall and utilize previously discussed information for natural dialogue. It then details three types: short-term (or working memory), which for AI is handled by a \"context window\" for immediate recall of recent turns; long-term (or episodic memory for AI), which involves recalling information from earlier in a single conversation or from past, separate conversations, often requiring external mechanisms like databases, summarization, or user profiles; and semantic memory (general knowledge), which is the AI\\'s pre-trained understanding of facts and concepts. The AI concludes that improving conversational memory is crucial for making AI assistants more natural and helpful, and asks James if there\\'s a particular aspect he\\'s most interested in. James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI confirms these are common implementations of short-term memory. Regarding `ConversationBufferMemory`, the AI confirms it stores the **entire, unedited transcript** of the conversation from the very beginning, like a complete log, to ensure the AI has access to every piece of information exchanged. However, it reiterates and elaborates on the significant considerations: the finite \"context window\" of LLMs means older parts are truncated if the transcript exceeds it (likened to fitting a long book into a small reading frame); increased cost due to more tokens processed; higher latency from processing larger amounts of text; and the potential for irrelevance as older, unneeded information is still sent, making it harder for the model to focus. Therefore, it\\'s best suited for shorter, focused conversations where full context is genuinely needed and risks are minimal. In contrast, `ConversationBufferWindowMemory` stores only the **last `N` turns** (a \"sliding window\"), which efficiently manages the context window and focuses on recent information, but at the cost of losing older context. The AI summarizes that the former is like a full, growing transcript (good for short chats, prone to limits), while the latter is a sliding window of recent turns (essential for long chats, but loses older info), and asks James if he\\'s interested in a specific framework or the conceptual trade-offs.',\n",
       " 'response': 'That\\'s absolutely right, James! You\\'ve hit the nail on the head with `ConversationBufferWindowMemory`.\\n\\nIt functions precisely as you described: it maintains a \"sliding window\" of the most recent `k` messages or turns in the conversation. Once a new message comes in, if the total number of turns exceeds `k`, the oldest turn is dropped from the memory, ensuring that the window always contains only the `k` most recent exchanges.\\n\\nThis design is incredibly efficient for managing the finite context window of large language models. By only keeping the latest interactions, it significantly reduces the amount of text that needs to be processed with each new turn, which in turn helps to:\\n*   **Control costs:** Fewer tokens mean lower processing expenses.\\n*   **Improve latency:** Less text to process means faster response times.\\n*   **Maintain focus:** The model is primarily exposed to the most recent and likely most relevant parts of the ongoing dialogue, preventing it from getting distracted by very old, potentially irrelevant information.\\n\\nHowever, as we touched upon, the crucial implication is that any information from turns older than `k` is effectively \"forgotten\" by the model\\'s immediate conversational memory. If a user refers back to something discussed 20 turns ago, but `k` is set to 10, the model won\\'t have that specific piece of information in its direct context window.\\n\\nIt\\'s a classic engineering trade-off: efficiency and focus versus complete historical recall.\\n\\nDoes that clarification align with your understanding, or are you perhaps wondering about how `k` is typically chosen, or in what specific scenarios this memory type truly shines compared to others?'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello there my name is James\"})\n",
    "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
    "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
    "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
    "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59fbf2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Current summary:\n",
      "James introduces himself to the AI. The AI introduces itself as a large language model trained by Google, designed for helpful conversations, answering questions, and generating creative content, and then asks James how he is and what he'd like to discuss. James states he is researching different types of conversational memory. The AI acknowledges this as a fascinating and important topic, defining conversational memory as the ability to recall and utilize previously discussed information for natural dialogue. It then details three types: short-term (or working memory), which for AI is handled by a \"context window\" for immediate recall of recent turns; long-term (or episodic memory for AI), which involves recalling information from earlier in a single conversation or from past, separate conversations, often requiring external mechanisms like databases, summarization, or user profiles; and semantic memory (general knowledge), which is the AI's pre-trained understanding of facts and concepts. The AI concludes that improving conversational memory is crucial for making AI assistants more natural and helpful, and asks James if there's a particular aspect he's most interested in. James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI confirms these are common implementations of short-term memory. Regarding `ConversationBufferMemory`, the AI confirms it stores the **entire, unedited transcript** of the conversation from the very beginning, like a complete log, to ensure the AI has access to every piece of information exchanged. However, it reiterates and elaborates on the significant considerations: the finite \"context window\" of LLMs means older parts are truncated if the transcript exceeds it (likened to fitting a long book into a small reading frame); increased cost due to more tokens processed; higher latency from processing larger amounts of text; and the potential for irrelevance as older, unneeded information is still sent, making it harder for the model to focus. Therefore, it's best suited for shorter, focused conversations where full context is genuinely needed and risks are minimal. In contrast, `ConversationBufferWindowMemory` stores only the **last `N` turns** (a \"sliding window\"), which efficiently manages the context window and focuses on recent information, but at the cost of losing older context. The AI summarizes that the former is like a full, growing transcript (good for short chats, prone to limits), while the latter is a sliding window of recent turns (essential for long chats, but loses older info), and asks James if he's interested in a specific framework or the conceptual trade-offs. James then confirms his understanding of `Buffer window memory`, stating it stores the last `k` messages and drops the rest. The AI affirms this is precisely how `ConversationBufferWindowMemory` functions, maintaining a \"sliding window\" of the most recent `k` messages and dropping the oldest when a new one arrives. It elaborates that this design is highly efficient for managing LLM context windows, as it reduces processed text, thereby controlling costs, improving latency, and maintaining the model's focus on recent, relevant dialogue. However, the AI reiterates that information from turns older than `k` is effectively \"forgotten\" from the model's immediate conversational memory, emphasizing this as a classic engineering trade-off between efficiency/focus and complete historical recall. The AI then asks James if this clarification aligns with his understanding or if he's interested in how `k` is typically chosen or in what specific scenarios this memory type truly shines.\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': 'Current summary:\\nJames introduces himself to the AI. The AI introduces itself as a large language model trained by Google, designed for helpful conversations, answering questions, and generating creative content, and then asks James how he is and what he\\'d like to discuss. James states he is researching different types of conversational memory. The AI acknowledges this as a fascinating and important topic, defining conversational memory as the ability to recall and utilize previously discussed information for natural dialogue. It then details three types: short-term (or working memory), which for AI is handled by a \"context window\" for immediate recall of recent turns; long-term (or episodic memory for AI), which involves recalling information from earlier in a single conversation or from past, separate conversations, often requiring external mechanisms like databases, summarization, or user profiles; and semantic memory (general knowledge), which is the AI\\'s pre-trained understanding of facts and concepts. The AI concludes that improving conversational memory is crucial for making AI assistants more natural and helpful, and asks James if there\\'s a particular aspect he\\'s most interested in. James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI confirms these are common implementations of short-term memory. Regarding `ConversationBufferMemory`, the AI confirms it stores the **entire, unedited transcript** of the conversation from the very beginning, like a complete log, to ensure the AI has access to every piece of information exchanged. However, it reiterates and elaborates on the significant considerations: the finite \"context window\" of LLMs means older parts are truncated if the transcript exceeds it (likened to fitting a long book into a small reading frame); increased cost due to more tokens processed; higher latency from processing larger amounts of text; and the potential for irrelevance as older, unneeded information is still sent, making it harder for the model to focus. Therefore, it\\'s best suited for shorter, focused conversations where full context is genuinely needed and risks are minimal. In contrast, `ConversationBufferWindowMemory` stores only the **last `N` turns** (a \"sliding window\"), which efficiently manages the context window and focuses on recent information, but at the cost of losing older context. The AI summarizes that the former is like a full, growing transcript (good for short chats, prone to limits), while the latter is a sliding window of recent turns (essential for long chats, but loses older info), and asks James if he\\'s interested in a specific framework or the conceptual trade-offs. James then confirms his understanding of `Buffer window memory`, stating it stores the last `k` messages and drops the rest. The AI affirms this is precisely how `ConversationBufferWindowMemory` functions, maintaining a \"sliding window\" of the most recent `k` messages and dropping the oldest when a new one arrives. It elaborates that this design is highly efficient for managing LLM context windows, as it reduces processed text, thereby controlling costs, improving latency, and maintaining the model\\'s focus on recent, relevant dialogue. However, the AI reiterates that information from turns older than `k` is effectively \"forgotten\" from the model\\'s immediate conversational memory, emphasizing this as a classic engineering trade-off between efficiency/focus and complete historical recall. The AI then asks James if this clarification aligns with his understanding or if he\\'s interested in how `k` is typically chosen or in what specific scenarios this memory type truly shines.',\n",
       " 'response': \"Ah, yes! Your name is James. It's a pleasure to continue our conversation, James!\\n\\nIs there anything else I can help you with regarding conversational memory, or perhaps something else entirely?\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1b35ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2485436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatGoogleGenerativeAI\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages and summarize the conversation.\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"You are a conversation summarizer.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Current Summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New Messages:\\n{new_messages}\\n\\n\"\n",
    "                \"Update the summary with relevant context.\"\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        existing_summary = \"\"\n",
    "        for msg in self.messages:\n",
    "            if isinstance(msg, SystemMessage):\n",
    "                existing_summary = msg.content\n",
    "                break\n",
    "\n",
    "        # Format new messages\n",
    "        new_msgs_text = \"\\n\".join(f\"{type(m).__name__}: {m.content}\" for m in messages)\n",
    "\n",
    "        formatted = summary_prompt.format_messages(\n",
    "            existing_summary=existing_summary,\n",
    "            new_messages=new_msgs_text\n",
    "        )\n",
    "\n",
    "        # Get new summary from Gemini\n",
    "        response = self.llm.invoke(formatted)\n",
    "\n",
    "        # Replace history with single SystemMessage (summary)\n",
    "        self.messages = [SystemMessage(content=response.content)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d95a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d933d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatGoogleGenerativeAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5baad7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it's good to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--fa5887a5-2fa3-42ca-8d7e-8a1d09459c47-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 50, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 17}})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87d36a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Current Summary:\\nJames introduced himself to the AI, which greeted him and offered assistance.\\n\\nNew Messages:\\nJames is researching different types of conversational memory. The AI explained that conversational memory is a complex interplay of cognitive processes and data structures, categorizing it from both a human cognitive perspective (Short-Term/Working, Long-Term/Episodic, Goal/Task, Personal/Relational, and Semantic Memory) and how AI systems implement or mimic these functions (Context Window/Buffer, Session History/Logs, Knowledge Bases & Databases, State Tracking, Embeddings & Vector Databases, and Summarization & Compression).', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")\n",
    "\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b03e37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d452df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='James introduced himself to the AI, which greeted him and offered assistance. James is researching different types of conversational memory. The AI explained that conversational memory involves cognitive processes and data structures, categorizing it from both a human cognitive perspective and how AI systems implement these functions. James then specifically asked about `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI explained that `ConversationBufferMemory` stores all previous messages, providing full context but facing scalability issues with token limits, cost, and latency for longer conversations. James confirmed his understanding that `ConversationBufferMemory` stores the entire conversation, which the AI affirmed, reiterating its full context provision but also its resource-intensive nature for very long conversations due to token limits, cost, and latency. In contrast, `ConversationBufferWindowMemory` stores only the last `N` messages, managing token limits and improving efficiency but losing older context. James confirmed his understanding that `ConversationBufferWindowMemory` stores only the last `k` messages, dropping older ones, which the AI affirmed, explaining it creates a \"window\" of recent conversation. `ConversationBufferMemory` is suited for shorter interactions, while `ConversationBufferWindowMemory` is more practical for longer conversations where managing token limits is crucial.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625cf66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab70f26b",
   "metadata": {},
   "source": [
    "ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf62f6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_15472\\569741439.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=300,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d1fecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: Hi, my name is James\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi, my name is James',\n",
       " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello James! It's really nice to meet you. I'm an AI, a large language model, created by Google. You can just call me AI, or whatever feels natural to you! I don't have a personal name like humans do, but I'm here and ready to chat about all sorts of things. How can I help you today, or what's on your mind?\", additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"Hello James! It's really nice to meet you. I'm an AI, a large language model, created by Google. You can just call me AI, or whatever feels natural to you! I don't have a personal name like humans do, but I'm here and ready to chat about all sorts of things. How can I help you today, or what's on your mind?\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\": \"Hi, my name is James\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b1484be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello James! It's really nice to meet you. I'm an AI, a large language model, created by Google. You can just call me AI, or whatever feels natural to you! I don't have a personal name like humans do, but I'm here and ready to chat about all sorts of things. How can I help you today, or what's on your mind?\", additional_kwargs={}, response_metadata={})]\n",
      "Human: I'm researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content='James introduces himself and states he is researching conversational memory. The AI introduces itself as a Google-created large language model and explains that conversational memory is crucial for coherent interactions, defining it as the ability to retain and recall information from previous turns. The AI then details five main types: Short-Term Memory (context window), Long-Term Memory (personalized/user profile), Semantic Memory (general knowledge base), Episodic Memory (specific events/interactions), and Dialogue State Tracking (a mechanism to understand conversation state and goals).', additional_kwargs={}, response_metadata={})]\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content='James introduces himself and states he is researching conversational memory. The AI introduces itself as a Google-created large language model and explains that conversational memory is crucial for coherent interactions, defining it as the ability to retain and recall information from previous turns. The AI then details five main types: Short-Term Memory (context window), Long-Term Memory (personalized/user profile), Semantic Memory (general knowledge base), Episodic Memory (specific events/interactions), and Dialogue State Tracking (a mechanism to understand conversation state and goals). The human then asks about `ConversationBufferMemory` and `ConversationBufferWindowMemory`, which the AI identifies as practical implementations of Short-Term Memory.\\n\\nThe AI explains `ConversationBufferMemory` as a complete, unedited transcript of the entire conversation, appending new messages. Its advantage is full context and high coherence, but its disadvantages include hitting token limits, high computational cost, and processing irrelevant information as the conversation grows. In contrast, `ConversationBufferWindowMemory` only retains the most recent N messages (a sliding window). This approach manages token limits, improves efficiency, and keeps the focus on recent topics, but its main drawback is the loss of older context, potentially leading to disjointedness if older topics are revisited. The AI summarizes that `ConversationBufferMemory` aims for complete recall but struggles with scalability, while `ConversationBufferWindowMemory` prioritizes scalability and efficiency by sacrificing some long-term recall within the current conversation, with developers choosing between them based on the expected conversation length and nature.', additional_kwargs={}, response_metadata={})]\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content='James introduces himself and states he is researching conversational memory. The AI introduces itself as a Google-created large language model and explains that conversational memory is crucial for coherent interactions, defining it as the ability to retain and recall information from previous turns. The AI then details five main types: Short-Term Memory (context window), Long-Term Memory (personalized/user profile), Semantic Memory (general knowledge base), Episodic Memory (specific events/interactions), and Dialogue State Tracking (a mechanism to understand conversation state and goals). The human then asks about `ConversationBufferMemory` and `ConversationBufferWindowMemory`, which the AI identifies as practical implementations of Short-Term Memory.\\n\\nThe AI explains `ConversationBufferMemory` as a complete, unedited transcript of the entire conversation, appending new messages. Its advantage is full context and high coherence, but its disadvantages include hitting token limits, high computational cost, and processing irrelevant information as the conversation grows. In contrast, `ConversationBufferWindowMemory` only retains the most recent N messages (a sliding window). This approach manages token limits, improves efficiency, and keeps the focus on recent topics, but its main drawback is the loss of older context, potentially leading to disjointedness if older topics are revisited. The AI summarizes that `ConversationBufferMemory` aims for complete recall but struggles with scalability, while `ConversationBufferWindowMemory` prioritizes scalability and efficiency by sacrificing some long-term recall within the current conversation, with developers choosing between them based on the expected conversation length and nature.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}), AIMessage(content='That\\'s absolutely spot on, James! You\\'ve got it exactly right.\\n\\n`ConversationBufferMemory` does indeed store the *entire* conversation, from the very first message right up to the most recent one. Think of it like a complete, unedited transcript that just keeps growing. Every new message, whether it\\'s from you or from me, gets appended to the end of that ongoing record.\\n\\nIt\\'s designed to give the AI the fullest possible context of our interaction, ensuring that nothing from our current chat is forgotten. This is why I mentioned earlier that it\\'s a practical implementation of what we call \"Short-Term Memory\" within my architecture – it\\'s all about keeping everything we\\'ve said in this specific conversation readily available within my \"context window.\"\\n\\nThe main idea behind it is to maximize coherence and ensure I can always refer back to anything we\\'ve discussed, no matter how far back it was in *this* particular session. Of course, as we talked about, that full recall comes with its own set of considerations, especially regarding how much information can fit into that context window!', additional_kwargs={}, response_metadata={})]\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for msg in [\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    chain.invoke({\"input\": msg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd293d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a806db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatGoogleGenerativeAI = Field(default_factory=ChatGoogleGenerativeAI)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: ChatGoogleGenerativeAI, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages and summarizing the messages that we\n",
    "        drop.\n",
    "        \"\"\"\n",
    "        existing_summary: SystemMessage | None = None\n",
    "        old_messages: list[BaseMessage] | None = None\n",
    "        # see if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0)\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "        # check if we have too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping \"\n",
    "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
    "            # pull out the oldest messages...\n",
    "            old_messages = self.messages[:self.k]\n",
    "            # ...and keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k:]\n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            # if we have no old_messages, we have nothing to update in summary\n",
    "            return\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary,\n",
    "                old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "        print(f\">> New summary: {new_summary.content}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bb76445",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e570b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatGoogleGenerativeAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15f0cb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> No old messages to update summary with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi James, it's good to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--3c60c1cc-e03e-4916-b04a-5222a05a2006-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 50, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 17}})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    ")\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aef4e165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Message 1\n",
      "---\n",
      "\n",
      ">> No old messages to update summary with\n",
      "---\n",
      "Message 2\n",
      "---\n",
      "\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: The conversation begins with James introducing himself. He then states that he is researching the different types of conversational memory. The AI explains that \"conversational memory\" is a functional concept, not a single type, and details its manifestations in both humans and AI systems.\n",
      "\n",
      "For **human conversational memory**, the AI describes:\n",
      "*   **Working Memory:** For immediate comprehension and response.\n",
      "*   **Episodic Memory:** For recalling specific events and past conversations.\n",
      "*   **Semantic Memory:** For general knowledge and understanding content.\n",
      "*   **Prospective Memory:** For remembering future intentions or actions.\n",
      "*   **Social/Pragmatic Memory:** For recalling social cues and norms.\n",
      "\n",
      "For **AI conversational memory**, the AI outlines:\n",
      "*   **Short-Term Context (Context Window):** A limited buffer of recent turns for immediate coherence.\n",
      "*   **Long-Term Memory (Knowledge Bases/Vector Databases/User Profiles):** For persistent information, often using Retrieval Augmented Generation (RAG).\n",
      "*   **Session Memory:** Information retained only for the duration of a single conversation session.\n",
      "*   **Global Memory:** Information applicable across all users or sessions, part of core training data.\n",
      "\n",
      "The AI concludes by reiterating that conversational memory leverages different underlying memory systems in humans and various technical mechanisms in AI, and asks James what aspect he is most interested in exploring further.\n",
      "---\n",
      "Message 3\n",
      "---\n",
      "\n",
      ">> Found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: The conversation begins with James introducing himself and stating his research interest in different types of conversational memory. The AI provides a comprehensive overview, explaining that \"conversational memory\" is a functional concept, not a single type, and details its manifestations in both humans (Working, Episodic, Semantic, Prospective, and Social/Pragmatic Memory) and AI systems (Short-Term Context, Long-Term Memory, Session Memory, and Global Memory).\n",
      "\n",
      "James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`, which the AI identifies as practical implementations within the LangChain framework for managing chat history.\n",
      "\n",
      "The AI explains:\n",
      "*   **`ConversationBufferMemory`**: This type stores *all* previous messages (user inputs and AI responses) in a buffer. It provides full context but is limited by LLM token limits, potentially leading to errors, increased costs, and higher latency. It is best suited for short, concise conversations.\n",
      "*   **`ConversationBufferWindowMemory`**: This type stores only the *last `k` messages* using a \"sliding window\" approach. It effectively manages token limits, reduces costs, and offers faster processing. However, its main drawback is the loss of older context beyond the `k` window. It is ideal for long-running conversations where maintaining a fixed context size and immediate past relevance are crucial.\n",
      "\n",
      "The AI clarifies that both `ConversationBufferMemory` and `ConversationBufferWindowMemory` primarily implement the **Short-Term Context / Dialogue History** aspect of AI conversational memory. It notes that they do not inherently provide **Long-Term Memory**, for which LangChain offers other memory types like `ConversationSummaryMemory` or `VectorStoreRetrieverMemory`. The conversation concludes with the AI asking James about specific scenarios where he might consider using these memory types.\n",
      "---\n",
      "Message 4\n",
      "---\n",
      "\n",
      ">> Found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: The conversation begins with James introducing himself and stating his research interest in different types of conversational memory. The AI provides a comprehensive overview, explaining that \"conversational memory\" is a functional concept, not a single type, and details its manifestations in both humans (Working, Episodic, Semantic, Prospective, and Social/Pragmatic Memory) and AI systems (Short-Term Context, Long-Term Memory, Session Memory, and Global Memory).\n",
      "\n",
      "James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`, which the AI identifies as practical implementations within the LangChain framework for managing chat history. Both types are typically integrated into LangChain `Chain`s (e.g., `ConversationChain`) to automatically include the relevant history in the prompt sent to the LLM.\n",
      "\n",
      "The AI explains:\n",
      "*   **`ConversationBufferMemory`**: This type stores *all* previous messages (user inputs and AI responses) in a buffer. It works by appending every new turn to a growing list and passing the entire accumulated history to the LLM, analogous to an ever-growing transcript. It provides full context and is simple to implement. However, its main drawbacks are hitting LLM token limits (potentially leading to errors or truncation), increased costs, and higher latency. It is best suited for short, concise conversations or debugging/testing scenarios.\n",
      "*   **`ConversationBufferWindowMemory`**: This type stores only the *last `k` messages* using a \"sliding window\" approach, where the oldest messages are discarded as new ones are added to maintain a fixed size, analogous to a notepad with limited lines. It effectively manages token limits, reduces costs, and offers faster processing. Its main drawback is the permanent loss of older context beyond the `k` window, meaning the LLM won't \"remember\" distant past information. Choosing the right `k` value can also be tricky. It is ideal for long-running conversations, chatbots where immediate past relevance is crucial, or scenarios requiring a balance between context retention, performance, and cost.\n",
      "\n",
      "The AI clarifies that both `ConversationBufferMemory` and `ConversationBufferWindowMemory` primarily implement the **Short-Term Context / Dialogue History** aspect of AI conversational memory. It notes that they do not inherently provide **Long-Term Memory**, for which LangChain offers other memory types like `ConversationSummaryMemory` or `VectorStoreRetrieverMemory`, or custom integrations. The conversation concludes with the AI asking James about specific scenarios where he might consider using these memory types, and James confirms his understanding that `ConversationBufferMemory` simply stores the entire conversation.\n"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate([\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89543e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
