{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b254f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"Give me a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e0cc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:502: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini_api =\"AIzaSyDaM0twsGt6Rv5M2pY4ze4lZlKc7IQWuiQ\"\n",
    "\n",
    "# Directly pass API key to avoid ADC errors\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key= gemini_api,\n",
    "    temperature = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fae3bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_10588\\288062190.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65caa897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'retrieval augmented generation',\n",
       " 'text': '## Small Report: Retrieval Augmented Generation (RAG)\\n\\n**Title:** Retrieval Augmented Generation (RAG): Enhancing LLM Accuracy and Relevance\\n\\n**Introduction:**\\nRetrieval Augmented Generation (RAG) is a paradigm-shifting technique designed to enhance the capabilities of Large Language Models (LLMs) by providing them with access to external, up-to-date, and domain-specific information. While LLMs are powerful in generating human-like text, they often suffer from \"hallucinations\" (generating factually incorrect information), outdated knowledge, or a lack of specific context for niche queries. RAG addresses these limitations by combining the generative power of LLMs with the precision of information retrieval systems.\\n\\n**How it Works:**\\nRAG operates in three primary steps:\\n\\n1.  **Retrieval:** When a user poses a query, the RAG system first searches a vast external knowledge base (e.g., a database of documents, articles, internal company data, or the internet). It identifies and retrieves the most relevant pieces of information, often called \"chunks\" or \"documents,\" that are pertinent to the query. This knowledge base can be continuously updated, ensuring the information is current.\\n\\n2.  **Augmentation:** The retrieved information is then used to \"augment\" or enrich the original user query. Instead of just sending the raw query to the LLM, the system constructs a new, more comprehensive prompt that includes both the user\\'s question and the relevant context retrieved from the external source.\\n\\n3.  **Generation:** Finally, this augmented prompt is fed into the LLM. The LLM then generates a response, but critically, it does so by grounding its answer in the provided retrieved context. This significantly reduces the likelihood of hallucinations and ensures the response is factual, relevant, and specific to the information found.\\n\\n**Key Benefits:**\\n\\n*   **Enhanced Accuracy & Factuality:** Reduces the risk of LLMs generating incorrect or fabricated information by providing verifiable sources.\\n*   **Access to Up-to-Date Information:** Bypasses the LLM\\'s training data cutoff, allowing it to incorporate real-time or frequently updated data.\\n*   **Domain Specificity:** Enables LLMs to answer questions about proprietary, internal, or highly specialized knowledge that wasn\\'t part of their original training.\\n*   **Reduced Training Costs:** Eliminates the need for expensive and time-consuming retraining (fine-tuning) of LLMs every time new information becomes available.\\n*   **Transparency & Attribution:** Can often provide citations or links to the source documents from which the information was retrieved, increasing user trust.\\n*   **Improved Explainability:** Makes it easier to understand *why* an LLM generated a particular answer, as the source context is explicit.\\n\\n**Challenges & Considerations:**\\n\\n*   **Retrieval Quality:** The effectiveness of RAG heavily depends on the quality of the retrieval step. Poorly retrieved information will lead to poor generation (\"garbage in, garbage out\").\\n*   **Context Window Limits:** LLMs have limits on how much text they can process in a single prompt, which can restrict the amount of retrieved context.\\n*   **Complexity:** Building and maintaining a robust RAG system, including data indexing, chunking strategies, and retrieval algorithms, can be complex.\\n*   **Latency:** The retrieval step adds an extra processing layer, potentially increasing response times compared to a standalone LLM.\\n\\n**Applications:**\\nRAG is being widely adopted across various sectors, including:\\n\\n*   **Customer Support:** Providing accurate answers from extensive knowledge bases.\\n*   **Enterprise Search:** Enabling employees to quickly find specific information within internal documents.\\n*   **Healthcare:** Answering medical queries based on the latest research and patient records.\\n*   **Legal:** Summarizing case law and retrieving relevant statutes.\\n*   **Education:** Creating personalized learning experiences with up-to-date content.\\n\\n**Conclusion:**\\nRetrieval Augmented Generation represents a significant leap forward in making LLMs more reliable, trustworthy, and practical for real-world applications. By intelligently combining the strengths of information retrieval with the generative power of large language models, RAG mitigates key limitations of standalone LLMs, paving the way for more accurate, contextually relevant, and verifiable AI-driven solutions.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af4a77",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac65149",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a16a1628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='## Small Report: Retrieval Augmented Generation (RAG)\\n\\n**Title:** Retrieval Augmented Generation (RAG): Enhancing LLM Accuracy and Relevance\\n\\n**Introduction:**\\nRetrieval Augmented Generation (RAG) is a paradigm-shifting technique designed to enhance the capabilities of Large Language Models (LLMs) by providing them with access to external, up-to-date, and domain-specific information. While LLMs are powerful in generating human-like text, they often suffer from \"hallucinations\" (generating factually incorrect information), outdated knowledge, or a lack of specific context for niche queries. RAG addresses these limitations by combining the generative power of LLMs with the precision of information retrieval systems.\\n\\n**How it Works:**\\nRAG operates in three primary steps:\\n\\n1.  **Retrieval:** When a user poses a query, the RAG system first searches a vast external knowledge base (e.g., a database of documents, articles, internal company data, or the internet). It identifies and retrieves the most relevant pieces of information, often called \"chunks\" or \"documents,\" that are pertinent to the query. This knowledge base can be continuously updated, ensuring the information is current.\\n\\n2.  **Augmentation:** The retrieved information is then used to \"augment\" or enrich the original user query. Instead of just sending the raw query to the LLM, the system constructs a new, more comprehensive prompt that includes both the user\\'s question and the relevant context retrieved from the external source.\\n\\n3.  **Generation:** Finally, this augmented prompt is fed into the LLM. The LLM then generates a response, but critically, it does so by grounding its answer in the provided retrieved context. This significantly reduces the likelihood of hallucinations and ensures the response is factual, relevant, and specific to the information found.\\n\\n**Key Benefits:**\\n\\n*   **Enhanced Accuracy & Factuality:** Reduces the risk of LLMs generating incorrect or fabricated information by providing verifiable sources.\\n*   **Access to Up-to-Date Information:** Bypasses the LLM\\'s training data cutoff, allowing it to incorporate real-time or frequently updated data.\\n*   **Domain Specificity:** Enables LLMs to answer questions about proprietary, internal, or highly specialized knowledge that wasn\\'t part of their original training.\\n*   **Reduced Training Costs:** Eliminates the need for expensive and time-consuming retraining (fine-tuning) of LLMs every time new information becomes available.\\n*   **Transparency & Attribution:** Can often provide citations or links to the source documents from which the information was retrieved, increasing user trust.\\n*   **Improved Explainability:** Makes it easier to understand *why* an LLM generated a particular answer, as the source context is explicit.\\n\\n**Challenges & Considerations:**\\n\\n*   **Retrieval Quality:** The effectiveness of RAG heavily depends on the quality of the retrieval step. Poorly retrieved information will lead to poor generation (\"garbage in, garbage out\").\\n*   **Context Window Limits:** LLMs have limits on how much text they can process in a single prompt, which can restrict the amount of retrieved context.\\n*   **Complexity:** Building and maintaining a robust RAG system, including data indexing, chunking strategies, and retrieval algorithms, can be complex.\\n*   **Latency:** The retrieval step adds an extra processing layer, potentially increasing response times compared to a standalone LLM.\\n\\n**Applications:**\\nRAG is being widely adopted across various sectors, including:\\n\\n*   **Customer Support:** Providing accurate answers from extensive knowledge bases.\\n*   **Enterprise Search:** Enabling employees to quickly find specific information within internal documents.\\n*   **Healthcare:** Answering medical queries based on the latest research and patient records.\\n*   **Legal:** Summarizing case law and retrieving relevant statutes.\\n*   **Education:** Creating personalized learning experiences with up-to-date content.\\n\\n**Conclusion:**\\nRetrieval Augmented Generation represents a significant leap forward in making LLMs more reliable, trustworthy, and practical for real-world applications. By intelligently combining the strengths of information retrieval with the generative power of large language models, RAG mitigates key limitations of standalone LLMs, paving the way for more accurate, contextually relevant, and verifiable AI-driven solutions.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--8e2424a2-c6a4-4c84-9efe-a4f68356513d-0', usage_metadata={'input_tokens': 10, 'output_tokens': 884, 'total_tokens': 1969, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1075}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e02fbc",
   "metadata": {},
   "source": [
    "How Does the Pipe Operator Work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c966fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other.invoke(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88c5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x+5\n",
    "\n",
    "def sub_five(x):\n",
    "    return x-5\n",
    "\n",
    "def mul_five(x):\n",
    "    return x*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9567f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "sub_five_runnable = Runnable(sub_five)\n",
    "mul_five_runnable = Runnable(mul_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb8f7e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
    "\n",
    "chain.invoke(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7251d343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
    "\n",
    "chain.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f514e6",
   "metadata": {},
   "source": [
    "RunnableLambda class is LangChain's built-in method for constructing a Runnable object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30c7dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "sub_five_runnable = RunnableLambda(sub_five)\n",
    "mul_five_runnable = RunnableLambda(mul_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa6df7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
    "chain.invoke(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c711f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "634189ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"give me a small report about {topic}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42d31559",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c33941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(\"AI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6681896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "old_word = \"AI\"\n",
    "new_word = \"AIRYS\"\n",
    "\n",
    "def replace_word(x):\n",
    "    return x.content.replace(old_word, new_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55adaedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_fact_runnable = RunnableLambda(extract_fact)\n",
    "replace_word_runnable = RunnableLambda(replace_word)\n",
    "\n",
    "chain = prompt | llm | extract_fact_runnable | replace_word_runnable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8e3953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(\"ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc5b0e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Artificial Intelligence (AIRYS): A Brief Report\\n\\n**Introduction**\\nArtificial Intelligence (AIRYS) is one of the most transformative technologies of our time, rapidly reshaping industries, economies, and daily life. At its core, AIRYS refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and understanding language.\\n\\n**What is AIRYS?**\\nAIRYS encompasses a broad range of technologies and techniques that enable machines to perform tasks that typically require human cognitive abilities. Rather than being explicitly programmed for every scenario, AIRYS systems are designed to learn from data, identify patterns, make predictions, and adapt their behavior.\\n\\n**How AIRYS Works (Simplified)**\\nMost modern AIRYS systems, particularly those leveraging Machine Learning (ML) and Deep Learning (DL), operate by:\\n1.  **Data Input:** Ingesting vast amounts of data (text, images, audio, numbers).\\n2.  **Pattern Recognition:** Using algorithms to find correlations and patterns within that data.\\n3.  **Learning:** Adjusting their internal parameters based on these patterns to improve performance on a specific task.\\n4.  **Prediction/Action:** Applying what they\\'ve learned to new, unseen data to make predictions, classify information, or take actions.\\n\\n**Key Types of AIRYS**\\n*   **Narrow AIRYS (ANI) / Weak AIRYS:** This is the most common form of AIRYS today. It is designed and trained for a specific task. Examples include virtual assistants (Siri, Alexa), recommendation engines (Netflix, Amazon), facial recognition, and spam filters. It excels at its designated task but lacks broader cognitive abilities.\\n*   **Artificial General Intelligence (AGI) / Strong AIRYS:** This theoretical concept refers to AIRYS that possesses human-like cognitive abilities across a wide range of tasks, capable of learning, understanding, and applying knowledge to any intellectual task that a human can. AGI remains a subject of research and is not yet a reality.\\n*   **Artificial Superintelligence (ASI):** A hypothetical AIRYS that surpasses human intelligence and capabilities in virtually every field, including scientific creativity, general wisdom, and social skills.\\n\\n**Common Applications**\\nAIRYS is already integrated into numerous aspects of our lives:\\n*   **Healthcare:** Disease diagnosis, drug discovery, personalized treatment plans.\\n*   **Finance:** Fraud detection, algorithmic trading, credit scoring.\\n*   **Transportation:** Self-driving cars, traffic management.\\n*   **Customer Service:** Chatbots, virtual assistants.\\n*   **Entertainment:** Content recommendation systems, personalized advertising.\\n*   **Manufacturing:** Robotics, predictive maintenance.\\n*   **Education:** Personalized learning platforms.\\n\\n**Benefits of AIRYS**\\n*   **Increased Efficiency & Productivity:** Automates repetitive tasks, freeing up human workers for more complex activities.\\n*   **Enhanced Decision-Making:** Provides data-driven insights and predictions that can improve strategic choices.\\n*   **Innovation:** Drives the creation of new products, services, and business models.\\n*   **Problem Solving:** Can analyze vast datasets to solve complex problems beyond human capacity.\\n*   **Accessibility:** Improves accessibility for individuals with disabilities through technologies like voice recognition and translation.\\n\\n**Challenges and Considerations**\\nDespite its potential, AIRYS presents significant challenges:\\n*   **Ethical Concerns:** Bias in data leading to unfair outcomes, privacy issues, and accountability for AIRYS decisions.\\n*   **Job Displacement:** Potential for automation to displace human jobs in certain sectors.\\n*   **Complexity & Transparency:** Many advanced AIRYS models are \"black boxes,\" making it difficult to understand how they arrive at their conclusions.\\n*   **Security Risks:** AIRYS systems can be vulnerable to attacks or misuse.\\n*   **Regulation:** The need for appropriate governance and ethical guidelines to ensure responsible development and deployment.\\n\\n**Conclusion**\\nArtificial Intelligence is a rapidly evolving field with immense potential to revolutionize nearly every aspect of society. While offering unprecedented opportunities for progress and innovation, its responsible development, ethical considerations, and careful integration are crucial to ensure its benefits are maximized for all, while mitigating potential risks.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8efbc9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"half the info is here\",\n",
    "        \"DeepSeek-V3 was released in December 2024\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"the other half of the info is here\",\n",
    "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a800fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d828c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "643d463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \"context_b\": retriever_b, \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b42e6472",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = retrieval | prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b19bd75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The DeepSeek-V3 LLM, released in December, uses a mixture of experts architecture.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--666c0900-720c-4ee2-8f40-1aab93bfa77b-0', usage_metadata={'input_tokens': 117, 'output_tokens': 20, 'total_tokens': 347, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 210}})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\n",
    "    \"what architecture does the model DeepSeek released in december use?\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a76b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f99e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
